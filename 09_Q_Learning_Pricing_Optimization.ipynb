{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d94fbb-31da-43b3-b8b1-d02b9ccad29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Folder Structure Setup\n",
    "base_folder = './output/input_data_characterization/'\n",
    "\n",
    "folders = ['model_files', 'predictions', 'plots']\n",
    "for folder in folders:\n",
    "    os.makedirs(os.path.join(base_folder, folder), exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/input/revised_synthetic_data.csv')\n",
    "\n",
    "# Normalize the data for Q-Learning\n",
    "def normalize_data(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Corrected column names for normalization\n",
    "data['Normalized_Demand'] = normalize_data(data['Demand'])\n",
    "data['Normalized_Supply'] = normalize_data(data['Supply'])\n",
    "data['Normalized_Economic_Growth_Rate'] = normalize_data(data['Economic_Growth_Rate'])\n",
    "data['Normalized_Waste_Generation'] = normalize_data(data['Waste_Generation'])\n",
    "data['Normalized_Energy_Prices'] = normalize_data(data['Energy_Prices'])\n",
    "\n",
    "# Initialize Q-Learning parameters\n",
    "state_columns = ['Normalized_Demand', 'Normalized_Supply', 'Normalized_Economic_Growth_Rate', 'Normalized_Waste_Generation', 'Normalized_Energy_Prices']\n",
    "action_column = 'Pricing'\n",
    "\n",
    "# Increase Q-table size for higher granularity in state space (10 bins)\n",
    "state_space_size = (10,) * len(state_columns)  # Increased to 10 bins per state\n",
    "action_space_size = 10  # Increase action space for more pricing options\n",
    "\n",
    "# Create the Q-table\n",
    "Q_table = np.zeros(state_space_size + (action_space_size,))\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.05  # Tuned learning rate\n",
    "discount_factor = 0.9  # Tuned discount factor for balancing future rewards\n",
    "exploration_rate = 1.0\n",
    "exploration_decay = 0.995  # Slower decay to allow more exploration\n",
    "min_exploration_rate = 0.1\n",
    "episodes = 1000  # Increase training duration to 1000 episodes\n",
    "\n",
    "# Discretize the state space into 10 bins (increased granularity)\n",
    "def discretize_state(row, state_columns, state_space_size):\n",
    "    state = []\n",
    "    for col in state_columns:\n",
    "        value = row[col]\n",
    "        state_value = np.digitize(value, np.linspace(0, 1, state_space_size[0] + 1)) - 1\n",
    "        state_value = min(state_value, state_space_size[0] - 1)  # Ensure within bounds\n",
    "        state.append(state_value)\n",
    "    return tuple(state)\n",
    "\n",
    "# Get the action based on the current state using epsilon-greedy strategy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        return random.randint(0, action_space_size - 1)  # Random action\n",
    "    else:\n",
    "        return np.argmax(Q_table[state])  # Best action\n",
    "\n",
    "# Improved reward function\n",
    "def calculate_reward(row):\n",
    "    demand_diff = abs(row['Demand'] - row['Pricing'])\n",
    "    supply_diff = abs(row['Supply'] - row['Pricing'])\n",
    "    # Penalize more when pricing deviates significantly from both demand and supply\n",
    "    return - ((demand_diff + supply_diff) ** 2)  # Square differences to penalize large deviations\n",
    "\n",
    "# Update Q-values\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    best_next_action = np.argmax(Q_table[next_state])\n",
    "    td_target = reward + discount_factor * Q_table[next_state + (best_next_action,)]\n",
    "    Q_table[state + (action,)] += learning_rate * (td_target - Q_table[state + (action,)])\n",
    "\n",
    "# Training the Q-learning model\n",
    "for episode in range(episodes):\n",
    "    print(f\"Episode {episode+1}/{episodes}\")\n",
    "    for index, row in data.iterrows():\n",
    "        state = discretize_state(row, state_columns, state_space_size)\n",
    "        action = choose_action(state)\n",
    "        reward = calculate_reward(row)\n",
    "\n",
    "        if index + 1 < len(data):\n",
    "            next_row = data.iloc[index + 1]\n",
    "            next_state = discretize_state(next_row, state_columns, state_space_size)\n",
    "            update_q_table(state, action, reward, next_state)\n",
    "\n",
    "    # Reduce exploration rate\n",
    "    exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "\n",
    "# Save the Q-table\n",
    "np.save(os.path.join(base_folder, 'model_files', 'Q_table.npy'), Q_table)\n",
    "\n",
    "# Extract optimized pricing actions from the Q-table\n",
    "data['Optimized_Pricing'] = data.apply(lambda row: np.argmax(Q_table[discretize_state(row, state_columns, state_space_size)]), axis=1)\n",
    "\n",
    "# Save the predictions to CSV\n",
    "data[['Month', 'Pricing', 'Optimized_Pricing']].to_csv(os.path.join(base_folder, 'predictions', 'Q_Learning_Pricing_Optimization.csv'), index=False)\n",
    "\n",
    "# Performance metrics calculation\n",
    "mae = mean_absolute_error(data['Pricing'], data['Optimized_Pricing'])\n",
    "mse = mean_squared_error(data['Pricing'], data['Optimized_Pricing'])\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(data['Pricing'], data['Optimized_Pricing'])\n",
    "\n",
    "performance_df = pd.DataFrame({\n",
    "    'MAE': [mae],\n",
    "    'MSE': [mse],\n",
    "    'RMSE': [rmse],\n",
    "    'R²': [r2]\n",
    "})\n",
    "\n",
    "# Save performance metrics to a file\n",
    "performance_df.to_csv(os.path.join(base_folder, 'predictions', 'Q_Learning_Performance_Metrics.csv'), index=False)\n",
    "\n",
    "# Resampling the data for plotting\n",
    "data['Month'] = pd.to_datetime(data['Month'])\n",
    "data.set_index('Month', inplace=True)\n",
    "data_resampled = data.resample('3M').mean()\n",
    "\n",
    "# Plot Pricing Optimization (following DQN-style plotting)\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.plot(data_resampled.index, data_resampled['Pricing'], label='Original Pricing', color='blue', linewidth=2)\n",
    "plt.plot(data_resampled.index, data_resampled['Optimized_Pricing'], label='Optimized Pricing', color='green', linestyle='--', linewidth=2)\n",
    "plt.title(\"Q-Learning: Actual vs Optimized Pricing\", fontsize=25)\n",
    "plt.xlabel(\"Date\", fontsize=32)\n",
    "plt.ylabel(\"Pricing\", fontsize=32)\n",
    "plt.legend(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the x-tick values (Yearly)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(base_folder, 'plots', 'Q_Learning_Actual_vs_Optimized_Pricing.pdf'), format='pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot Pricing Residuals (following DQN-style plotting)\n",
    "residuals = data_resampled['Pricing'] - data_resampled['Optimized_Pricing']\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.plot(data_resampled.index, residuals, label='Residuals', color='red', linewidth=2)\n",
    "plt.title(\"Q-Learning: Pricing Residuals\", fontsize=25)\n",
    "plt.xlabel(\"Date\", fontsize=32)\n",
    "plt.ylabel(\"Residuals\", fontsize=32)\n",
    "plt.legend(fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the x-tick values (Yearly)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "# Save the residual plot\n",
    "plt.savefig(os.path.join(base_folder, 'plots', 'Q_Learning_Pricing_Residuals.pdf'), format='pdf')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Performance Metrics:\\nMAE: {mae}\\nMSE: {mse}\\nRMSE: {rmse}\\nR²: {r2}\")\n",
    "print(\"Q-Learning Pricing Optimization and Performance Evaluation Completed Successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

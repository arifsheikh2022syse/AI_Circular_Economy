{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651edf-50f2-4359-a157-15cfb1928c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Folder Structure Setup\n",
    "base_folder = './output/input_data_characterization/'\n",
    "\n",
    "folders = ['model_files', 'predictions', 'plots']\n",
    "for folder in folders:\n",
    "    os.makedirs(os.path.join(base_folder, folder), exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./data/input/revised_synthetic_data.csv')\n",
    "\n",
    "# Normalize the data for DQN\n",
    "def normalize_data(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "# Corrected column names for normalization\n",
    "data['Normalized_Demand'] = normalize_data(data['Demand'])\n",
    "data['Normalized_Supply'] = normalize_data(data['Supply'])\n",
    "data['Normalized_Economic_Growth_Rate'] = normalize_data(data['Economic_Growth_Rate'])\n",
    "data['Normalized_Waste_Generation'] = normalize_data(data['Waste_Generation'])\n",
    "data['Normalized_Energy_Prices'] = normalize_data(data['Energy_Prices'])\n",
    "\n",
    "# State columns and action column\n",
    "state_columns = ['Normalized_Demand', 'Normalized_Supply', 'Normalized_Economic_Growth_Rate', 'Normalized_Waste_Generation', 'Normalized_Energy_Prices']\n",
    "action_column = 'Pricing'\n",
    "\n",
    "# Define Q-learning and DQN parameters\n",
    "state_space_size = (3,) * len(state_columns)  # Reduce state space size for efficiency\n",
    "action_space_size = 3  # Reduce action space size for efficiency\n",
    "learning_rate = 0.001\n",
    "discount_factor = 0.95\n",
    "episodes = 10  # Increased to a reasonable number of episodes\n",
    "\n",
    "# Function to discretize the state space\n",
    "def discretize_state(row, state_columns, state_space_size):\n",
    "    state = []\n",
    "    for col in state_columns:\n",
    "        value = row[col]\n",
    "        state_value = np.digitize(value, np.linspace(0, 1, state_space_size[0] + 1)) - 1\n",
    "        state_value = min(state_value, state_space_size[0] - 1)\n",
    "        state.append(state_value)\n",
    "    return tuple(state)\n",
    "\n",
    "# Build DQN model with reduced layers and neurons\n",
    "def build_dqn_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(len(state_columns),)))\n",
    "    model.add(layers.Dense(64, activation='relu'))  # Moderate neurons\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(action_space_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model\n",
    "\n",
    "# Initialize DQN model\n",
    "model = build_dqn_model()\n",
    "target_model = build_dqn_model()  # Target network for stability\n",
    "target_model.set_weights(model.get_weights())  # Sync with main model\n",
    "\n",
    "# Experience replay buffer\n",
    "buffer = []\n",
    "buffer_capacity = 100  # Larger buffer\n",
    "batch_size = 16  # Moderate batch size for stable training\n",
    "\n",
    "# Function to choose action using epsilon-greedy strategy\n",
    "def choose_action(state, exploration_rate):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        return random.randint(0, action_space_size - 1)  # Random action\n",
    "    else:\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "# Function to update the model using experience replay\n",
    "def train_model():\n",
    "    if len(buffer) < batch_size:\n",
    "        return\n",
    "    mini_batch = random.sample(buffer, batch_size)\n",
    "    for state, action, reward, next_state, done in mini_batch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target_q = target_model.predict(np.expand_dims(next_state, axis=0), verbose=0)\n",
    "            target = reward + discount_factor * np.max(target_q)\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        q_values[0][action] = target\n",
    "        model.fit(np.expand_dims(state, axis=0), q_values, epochs=1, verbose=0)\n",
    "\n",
    "# Function to update the target network\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Training the DQN model\n",
    "exploration_rate = 1.0\n",
    "exploration_decay = 0.9  # Faster decay\n",
    "min_exploration_rate = 0.05  # Reduce min exploration\n",
    "sync_steps = 2  # Sync target model every 2 episodes\n",
    "\n",
    "for episode in range(episodes):\n",
    "    print(f\"Episode {episode+1}/{episodes}\")\n",
    "    for index, row in data.iterrows():\n",
    "        if index > 50:  # Limiting number of steps inside the episode to avoid long processing\n",
    "            break\n",
    "        \n",
    "        state = discretize_state(row, state_columns, state_space_size)\n",
    "        action = choose_action(state, exploration_rate)\n",
    "        reward = -abs(row[action_column] - row['Demand'])  # Reward function based on pricing difference\n",
    "        \n",
    "        done = index + 1 >= len(data)  # Episode ends at last data point\n",
    "        if not done:\n",
    "            next_row = data.iloc[index + 1]\n",
    "            next_state = discretize_state(next_row, state_columns, state_space_size)\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        # Store experience in buffer\n",
    "        buffer.append((state, action, reward, next_state, done))\n",
    "        if len(buffer) > buffer_capacity:\n",
    "            buffer.pop(0)\n",
    "        \n",
    "        # Train the model\n",
    "        train_model()\n",
    "\n",
    "    # Update exploration rate\n",
    "    exploration_rate = max(min_exploration_rate, exploration_rate * exploration_decay)\n",
    "    \n",
    "    # Sync target network\n",
    "    if episode % sync_steps == 0:\n",
    "        update_target_model()\n",
    "\n",
    "# Save model and predictions\n",
    "model.save(os.path.join(base_folder, 'model_files', 'DQN_model.h5'))\n",
    "\n",
    "# Extract optimized pricing actions\n",
    "data['Optimized_Pricing'] = data.apply(lambda row: np.argmax(model.predict(np.expand_dims(discretize_state(row, state_columns, state_space_size), axis=0), verbose=0)), axis=1)\n",
    "\n",
    "# Save predictions\n",
    "data[['Month', 'Pricing', 'Optimized_Pricing']].to_csv(os.path.join(base_folder, 'predictions', 'DQN_Pricing_Optimization.csv'), index=False)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mae = mean_absolute_error(data['Pricing'], data['Optimized_Pricing'])\n",
    "mse = mean_squared_error(data['Pricing'], data['Optimized_Pricing'])\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(data['Pricing'], data['Optimized_Pricing'])\n",
    "\n",
    "# Save performance metrics\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'MAE': [mae],\n",
    "    'MSE': [mse],\n",
    "    'RMSE': [rmse],\n",
    "    'R²': [r2]\n",
    "})\n",
    "performance_metrics.to_csv(os.path.join(base_folder, 'predictions', 'DQN_performance_metrics.csv'), index=False)\n",
    "\n",
    "# Ensure 'Month' is the DatetimeIndex before resampling\n",
    "if 'Month' in data.columns:\n",
    "    data['Month'] = pd.to_datetime(data['Month'])\n",
    "    data.set_index('Month', inplace=True)\n",
    "\n",
    "# Resampling to reduce granularity for plotting\n",
    "data_resampled = data.resample('3M').mean()\n",
    "\n",
    "# Plot Pricing Optimization\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.plot(data_resampled.index, data_resampled['Pricing'], label='Original Pricing', color='blue', linewidth=2)\n",
    "plt.plot(data_resampled.index, data_resampled['Optimized_Pricing'], label='Optimized Pricing', color='green', linestyle='--', linewidth=2)\n",
    "plt.title(\"DQN: Actual vs Optimized Pricing\", fontsize=25)\n",
    "plt.xlabel(\"Date\", fontsize=32)\n",
    "plt.ylabel(\"Pricing\", fontsize=32)\n",
    "plt.legend(fontsize=20)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the x-tick values (Yearly)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(base_folder, 'plots', 'DQN_Actual_vs_Optimized_Pricing.pdf'), format='pdf')\n",
    "plt.close()\n",
    "\n",
    "# Plot Pricing Residuals\n",
    "residuals = data_resampled['Pricing'] - data_resampled['Optimized_Pricing']\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.plot(data_resampled.index, residuals, label='Residuals', color='red', linewidth=2)\n",
    "plt.title(\"DQN: Pricing Residuals\", fontsize=25)\n",
    "plt.xlabel(\"Date\", fontsize=32)\n",
    "plt.ylabel(\"Residuals\", fontsize=32)\n",
    "plt.legend(fontsize=20)\n",
    "plt.grid(True)\n",
    "\n",
    "# Set the x-tick values (Yearly)\n",
    "plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "\n",
    "# Save the residual plot\n",
    "plt.savefig(os.path.join(base_folder, 'plots', 'DQN_Pricing_Residuals.pdf'), format='pdf')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Performance Metrics:\\nMAE: {mae}\\nMSE: {mse}\\nRMSE: {rmse}\\nR²: {r2}\")\n",
    "print(\"DQN Pricing Optimization and Performance Evaluation Completed Successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
